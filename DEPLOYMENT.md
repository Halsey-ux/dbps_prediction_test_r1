# ğŸš€ æ¶ˆæ¯’å‰¯äº§ç‰©é¢„æµ‹ç³»ç»Ÿ - éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®å½•
- [æœ¬åœ°éƒ¨ç½²](#æœ¬åœ°éƒ¨ç½²)
- [äº‘ç«¯éƒ¨ç½²](#äº‘ç«¯éƒ¨ç½²)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

## ğŸ  æœ¬åœ°éƒ¨ç½²

### 1. ç¯å¢ƒè¦æ±‚
- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- **å†…å­˜**: æœ€å°‘ 4GB RAMï¼ˆæ¨è 8GBï¼‰
- **å­˜å‚¨**: è‡³å°‘ 2GB å¯ç”¨ç©ºé—´
- **æ“ä½œç³»ç»Ÿ**: Windows 10+, macOS 10.14+, Ubuntu 18.04+

### 2. å¿«é€Ÿå¼€å§‹

#### æ–¹æ³•ä¸€ï¼šè‡ªåŠ¨å¯åŠ¨ï¼ˆæ¨èï¼‰
```bash
# 1. æ¿€æ´»condaç¯å¢ƒ
conda activate test_r1_env

# 2. è¿è¡Œè‡ªåŠ¨å¯åŠ¨è„šæœ¬
python run_app.py
```

#### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨å®‰è£…
```bash
# 1. æ¿€æ´»condaç¯å¢ƒ
conda activate test_r1_env

# 2. å®‰è£…Webåº”ç”¨ä¾èµ–
pip install streamlit plotly

# 3. å¯åŠ¨åº”ç”¨
streamlit run app.py
```

### 3. è®¿é—®åº”ç”¨
- ğŸŒ **æœ¬åœ°åœ°å€**: http://localhost:8501
- ğŸ“± **ç§»åŠ¨ç«¯**: åŒç½‘ç»œä¸‹å…¶ä»–è®¾å¤‡å¯é€šè¿‡æ‚¨çš„IPåœ°å€è®¿é—®

## â˜ï¸ äº‘ç«¯éƒ¨ç½²

### 1. Streamlit Cloud éƒ¨ç½²

#### æ­¥éª¤ï¼š
1. **å‡†å¤‡GitHubä»“åº“**
   ```bash
   # ç¡®ä¿ä»£ç å·²æ¨é€åˆ°GitHub
   git push origin main
   ```

2. **è®¿é—® Streamlit Cloud**
   - å‰å¾€ [share.streamlit.io](https://share.streamlit.io)
   - ä½¿ç”¨GitHubè´¦å·ç™»å½•

3. **åˆ›å»ºåº”ç”¨**
   - ç‚¹å‡» "New app"
   - é€‰æ‹©æ‚¨çš„GitHubä»“åº“
   - è®¾ç½®ä»¥ä¸‹å‚æ•°ï¼š
     - **Repository**: `your-username/dbps_prediction_test_r1`
     - **Branch**: `main`
     - **Main file path**: `app.py`

4. **é…ç½®ç¯å¢ƒ**
   - Streamlit Cloudä¼šè‡ªåŠ¨è¯†åˆ« `requirements.txt`
   - åº”ç”¨å°†è‡ªåŠ¨æ„å»ºå’Œéƒ¨ç½²

#### æ³¨æ„äº‹é¡¹ï¼š
- âš ï¸ **æ¨¡å‹æ–‡ä»¶**: éœ€è¦å…ˆè®­ç»ƒæ¨¡å‹å¹¶æäº¤ `.pth` å’Œ `.json` æ–‡ä»¶
- ğŸ“ **æ–‡ä»¶å¤§å°**: GitHubæœ‰100MBæ–‡ä»¶å¤§å°é™åˆ¶
- ğŸ”§ **ä¾èµ–ç®¡ç†**: ç¡®ä¿ `requirements.txt` åŒ…å«æ‰€æœ‰ä¾èµ–

### 2. Heroku éƒ¨ç½²

#### å‡†å¤‡æ–‡ä»¶ï¼š
1. **åˆ›å»º Procfile**
   ```bash
   echo "web: streamlit run app.py --server.port=\$PORT --server.address=0.0.0.0" > Procfile
   ```

2. **åˆ›å»º runtime.txt**
   ```bash
   echo "python-3.9.18" > runtime.txt
   ```

#### éƒ¨ç½²æ­¥éª¤ï¼š
```bash
# 1. å®‰è£…Heroku CLI
# è®¿é—®: https://devcenter.heroku.com/articles/heroku-cli

# 2. ç™»å½•Heroku
heroku login

# 3. åˆ›å»ºåº”ç”¨
heroku create your-app-name

# 4. æ¨é€ä»£ç 
git push heroku main

# 5. æ‰“å¼€åº”ç”¨
heroku open
```

### 3. Docker éƒ¨ç½²

#### åˆ›å»º Dockerfileï¼š
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å¤åˆ¶æ–‡ä»¶
COPY requirements.txt .
COPY . .

# å®‰è£…ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# æš´éœ²ç«¯å£
EXPOSE 8501

# å¥åº·æ£€æŸ¥
HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health

# å¯åŠ¨å‘½ä»¤
ENTRYPOINT ["streamlit", "run", "app.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

#### æ„å»ºå’Œè¿è¡Œï¼š
```bash
# æ„å»ºé•œåƒ
docker build -t dbps-prediction .

# è¿è¡Œå®¹å™¨
docker run -p 8501:8501 dbps-prediction
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æ¨¡å‹æ–‡ä»¶ç¼ºå¤±
**é—®é¢˜**: `æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# è®­ç»ƒæ¨¡å‹
python train.py

# æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
ls -la *.pth *.json
```

#### 2. ä¾èµ–å®‰è£…å¤±è´¥
**é—®é¢˜**: `Import "torch" could not be resolved`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é‡æ–°å®‰è£…PyTorch
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cpu

# é‡æ–°å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

#### 3. ç«¯å£è¢«å ç”¨
**é—®é¢˜**: `Port 8501 is already in use`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1: ä½¿ç”¨å…¶ä»–ç«¯å£
streamlit run app.py --server.port=8502

# æ–¹æ³•2: å…³é—­å ç”¨ç«¯å£çš„è¿›ç¨‹
# Windows:
netstat -ano | findstr :8501
taskkill /PID <PID> /F

# Linux/Mac:
lsof -ti:8501 | xargs kill
```

#### 4. å†…å­˜ä¸è¶³
**é—®é¢˜**: æ¨¡å‹åŠ è½½æ—¶å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
- å…³é—­å…¶ä»–åº”ç”¨ç¨‹åº
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®
- å¢åŠ ç³»ç»Ÿè™šæ‹Ÿå†…å­˜

#### 5. ç½‘ç»œè®¿é—®é—®é¢˜
**é—®é¢˜**: å±€åŸŸç½‘å†…å…¶ä»–è®¾å¤‡æ— æ³•è®¿é—®

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å…è®¸å¤–éƒ¨è®¿é—®
streamlit run app.py --server.address=0.0.0.0

# æ£€æŸ¥é˜²ç«å¢™è®¾ç½®
# å…è®¸8501ç«¯å£é€šè¿‡é˜²ç«å¢™
```

### æ€§èƒ½ä¼˜åŒ–

#### 1. æ¨¡å‹åŠ è½½ä¼˜åŒ–
```python
# åœ¨app.pyä¸­ä½¿ç”¨ç¼“å­˜
@st.cache_resource
def load_model():
    return ReactionPredictor(model_path, vocab_path)
```

#### 2. å†…å­˜ä¼˜åŒ–
```python
# æ¸…ç†GPUå†…å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
import torch
torch.cuda.empty_cache()
```

#### 3. å¹¶å‘å¤„ç†
```toml
# .streamlit/config.toml
[server]
maxUploadSize = 200
enableWebsocketCompression = true
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### å¯ç”¨æ—¥å¿—è®°å½•
```bash
# è¯¦ç»†æ—¥å¿—
streamlit run app.py --logger.level=debug

# ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
streamlit run app.py > app.log 2>&1
```

### æ€§èƒ½ç›‘æ§
- ä½¿ç”¨ `streamlit run app.py --server.enableStaticServing=true` æé«˜é™æ€æ–‡ä»¶æœåŠ¡æ€§èƒ½
- ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
- è®°å½•é¢„æµ‹å“åº”æ—¶é—´

## ğŸ” å®‰å…¨é…ç½®

### ç”Ÿäº§ç¯å¢ƒè®¾ç½®
```toml
# .streamlit/config.toml
[server]
enableCORS = false
enableXsrfProtection = true

[global]
developmentMode = false
```

### ç¯å¢ƒå˜é‡ç®¡ç†
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export STREAMLIT_SERVER_PORT=8501
export STREAMLIT_SERVER_ADDRESS=localhost
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š
1. æ£€æŸ¥æœ¬æ–‡æ¡£çš„æ•…éšœæ’é™¤éƒ¨åˆ†
2. æŸ¥çœ‹åº”ç”¨æ—¥å¿—æ–‡ä»¶
3. ç¡®è®¤æ‰€æœ‰ä¾èµ–å·²æ­£ç¡®å®‰è£…
4. éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§

**è”ç³»æ–¹å¼**: åˆ›å»ºGitHub Issueæˆ–æäº¤PR

---

ğŸ‰ **æ­å–œï¼æ‚¨çš„æ¶ˆæ¯’å‰¯äº§ç‰©é¢„æµ‹ç³»ç»Ÿå·²æˆåŠŸéƒ¨ç½²ï¼** 